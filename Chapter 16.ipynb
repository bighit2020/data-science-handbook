{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16장. 자연어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ pip3 install nltk\n",
    "```\n",
    "또는\n",
    "```\n",
    "$ sudo pip3 install nltk\n",
    "```\n",
    "로 nltk를 설치하고 실행하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 야후 서비스 종료로 코드는 작동하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/gnu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "긍정적인 기사: 0개, 부정적인 기사: 0개\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import urllib\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "TICKER = 'CSCO'  # 주식명 설정 \n",
    "URL_TEMPLATE = \"https://feeds.finance.yahoo.com/\" + \\\n",
    "   \"rss/2.0/headline?s=%s&region=US&lang=en-US\"\n",
    "\n",
    "nltk.download('stopwords')  # 불용어를 다운받는다\n",
    "\n",
    "def get_article_urls(ticker): \n",
    "    '''해당 주식과 관련된 기사를 반환한다.'''\n",
    "    link_pattern = re.compile(r\"<link>[^<]*</link>\")\n",
    "    xml_url = URL_TEMPLATE % ticker\n",
    "    xml_data = urllib.request.urlopen(xml_url).read().decode('utf-8')\n",
    "    link_hits = re.findall(link_pattern, xml_data)\n",
    "    return [h[6:-7] for h in link_hits]\n",
    "\n",
    "def get_article_content(url):\n",
    "    '''입력 : 신문 기사 url\n",
    "    출력 : 신문 기사 전처리 결과 \n",
    "    HTML파일을 다운받은 뒤,\n",
    "    각 문단의 내용을 정리한다.'''\n",
    "    paragraph_re = re.compile(r\"<p>.*</p>\")\n",
    "    tag_re = re.compile(r\"<[^>]*>\")\n",
    "    raw_html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    paragraphs = re.findall(paragraph_re, raw_html) \n",
    "    all_text = \"\".join(paragraphs)\n",
    "    content = re.sub(tag_re, \"\", all_text)\n",
    "    return content\n",
    "\n",
    "def text_to_bag(txt):\n",
    "    '''입력 : 문자열 \n",
    "    출력 : BoW(Bag-of-words) 특징값\n",
    "    불용어(stop words)는 무시하고 처리한다.'''\n",
    "    lemmatizer  = WordNetLemmatizer()\n",
    "    txt_as_ascii = txt.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(txt_as_ascii)\n",
    "    words = [t for t in tokens if t.isalpha()]\n",
    "    lemmas = [lemmatizer.lemmatize(w) for w in words]\n",
    "    stop = set(stopwords.words('english'))\n",
    "    nostops = [l for l in lemmas if l not in stop]\n",
    "    return nltk.FreqDist(nostops)\n",
    "\n",
    "def count_good_bad(bag):\n",
    "    '''입력 : BoW 특징값\n",
    "    출력 : 긍정/부정적 단어의 개수'''\n",
    "    good_synsets = set(wn.synsets('good') + \\\n",
    "        wn.synsets('up'))\n",
    "    bad_synsets = set(wn.synsets('bad') + \\\n",
    "        wn.synsets('down'))\n",
    "    n_good, n_bad = 0, 0\n",
    "    for lemma, ct in bag.items():\n",
    "        ss = wn.synsets(lemma)\n",
    "        if good_synsets.intersection(ss): n_good += 1\n",
    "        if bad_synsets.intersection(ss): n_bad += 1\n",
    "\n",
    "    return n_good, n_bad\n",
    "\n",
    "urls = get_article_urls(TICKER)\n",
    "contents = [get_article_content(u) for u in urls]\n",
    "bags = [text_to_bag(txt) for txt in contents]\n",
    "counts = [count_good_bad(txt) for txt in bags]\n",
    "n_good_articles = len([_ for g, b in counts if g > b]) # 긍정적인 기사 개수\n",
    "n_bad_articles = len([_ for g, b in counts if g < b]) # 부정적인 기사 개수\n",
    "\n",
    "print(\"긍정적인 기사: %i개, 부정적인 기사: %i개\" %\n",
    "    (n_good_articles, n_bad_articles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
