{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13장 빅데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.4 파이스파크\n",
    "\n",
    "## `myfile.txt`가 주어지지 않아서 실행은 안됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkContext 객체 생성\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# 파일의 각 행을 읽고 클러스터로 분산시켜서\n",
    "# Spark RDD를 만듦\n",
    "lines = open(\"myfile.txt\")\n",
    "lines_rdd = sc.parallelize(lines)\n",
    "\n",
    "# 문장 부호를 없애고 모든 행을 소문자로 변환\n",
    "def clean_line(s):\n",
    "    s2 = s.strip().lower()\n",
    "    s3 = s2.replace(\".\",\"\").replace(\",\",\"\")\n",
    "    return s3\n",
    "\n",
    "lines_clean = lines_rdd.map(clean_line)\n",
    "\n",
    "# 각 행을 단어 단위로 쪼갬\n",
    "words_rdd = lines_clean.flatmap(lambda l: l.split())\n",
    "\n",
    "# 각 단어의 등장 횟수를 셈\n",
    "def merge_counts(count1, count2):\n",
    "    return count1 + count2\n",
    "\n",
    "words_w_1 = words_rdd.map(lambda w: (w, 1))\n",
    "counts = words_w_1.reduceByKey(merge_counts)\n",
    "\n",
    "# 횟수를 취합하여 출력\n",
    "for word, count in counts.collect():\n",
    "    print(\"%s: %i \" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(line):\n",
    "    l2 = l.strip().lower()\n",
    "    l3  = l2.replace(\".\",\"\").replace(\",\",\"\")\n",
    "    words = l3.split()\n",
    "    return [(w, 1) for w in words]\n",
    "\n",
    "def reducer_func(count1, count2):\n",
    "    return count1 + count2\n",
    "\n",
    "lines = open(\"myfile.txt\")\n",
    "lines_rdd = sc.parallelize(lines)\n",
    "\n",
    "map_stage_out = lines_rdd.flatMap(mapper)\n",
    "reduce_stage_out = map_stage_out.reduceByKey(reducer_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(line):\n",
    "    return [(w, [w]) for w in line.split()]\n",
    "\n",
    "def red_func(lst1, lst2):\n",
    "    return lst1 + lst2\n",
    "\n",
    "result = lines.flatMap(mapper).reduceByKey(red_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_agg(agg_list, new_word):\n",
    "    agg_list.append(new_word)\n",
    "    return agg_list  # 동일한 리스트!\n",
    "\n",
    "def merge_agg_lists(agg_list1, agg_list2):\n",
    "    return agg_list1 + agg_list2\n",
    "\n",
    "result = lines.flatMap(mapper).aggregateByKey(\n",
    "    [], update_agg, merge_agg_lists)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
